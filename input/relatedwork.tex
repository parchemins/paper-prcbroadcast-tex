
\section{Related work}
\label{sec:relatedwork}

Causal broadcast ensures causal order and forbids multiple
delivery. \RPCBROADCAST uses the former to improve on the complexity of the
latter.  This section reviews state-of-the-art broadcast protocols that forbid
multiple delivery in asynchronous and dynamic systems.

Building \textbf{specific dissemination topologies} such as tree or ring
guarantees that every process receives each message
once~\cite{bravo2017saturn,raynal2013distributed}. Processes deliver messages as
soon as they arrive. They do not need to save any control information about
messages, for they will never receive a copy of this message again. While these
approaches are lightweight, they stay confined to systems where failures are
uncommon, and where churn rate remains
low~\cite{krasikova2016hashtable}. \RPCBROADCAST generalizes on these specific
topologies. It follows the same principle where the topology impacts on the
number of receipts. Its space complexity scales linearly with this number of
receipts. In turns, \RPCBROADCAST inherits from the resilience of the underlying
topology maintained by processes. \RPCBROADCAST supports dynamic systems without
assuming any specific topology.

\begin{table*}
  \begin{center}
    \caption{\label{table:complexity} Complexity of broadcast algorithms at each
      process. $N$ the number of processes that ever broadcast a message. $P$ is
      the set of processes in the system. $W$ the number of messages received
      but not delivered yet. $Q_i$ is the set of incoming links. $M$ is the
      number of messages already delivered that will be received again from at
      least one link in $Q_i$.}
  \input{input/tablecomplexity.tex}
  \end{center}
\end{table*}


Without any specific dissemination topology, each process may receive each
broadcast message multiple times. Despite multiple receipts, a process must
deliver a message once.  Using local structures based on \textbf{logical
  clocks~\cite{lamport1978time}}, every process differentiates between the first
receipt of a broadcast message and the additional receipts of this message. It
allows to deliver the former while ignoring the latter. Unfortunately, the size
of these structures increases monotonically and linearly with the number of
processes that ever broadcast a
message~\cite{malkhi2007concise,mukund2014optimized}.  Processes cannot reclaim
the space consumed, for it would require running an overcostly distributed
garbage collection that is equivalent to a distributed
consensus~\cite{abdullahi1998garbage}.  This limits their use to context where
the number of broadcasters is known to be small.  \RPCBROADCAST uses local
structures based on logical clocks too. However, instead of saving the past
deliveries of broadcasters, its saves the messages expected from direct
neighbors. The set of expected messages varies over receipts, and the number of
neighbors can be far smaller than the set of broadcasters. \RPCBROADCAST scales
in large and dynamic systems. Among others, \RPCBROADCAST fits contexts where
the number of participants is unknown, such as distributed collaborative
editing~\cite{nedelec2016crate}.

Table~\ref{table:complexity} summarizes the complexity of broadcast
implementations that handle asynchronous and dynamic systems. To the best of our
knowledge, all causal broadcast implementations use an underlying reliable
broadcast in order to forbid multiple delivery. Their local space complexity
comprises $O(N)$ where $N$ is the number of processes that ever broadcast a
message. Compared to preventive causal broadcast~\cite{nedelec2018pcbroadcast},
\RPCBROADCAST slightly increases the delivery execution time, and doubles the
number of control message per added link. In turns, \RPCBROADCAST keeps a
constant overhead on broadcast message, and changes the terms of local space
complexity. Most importantly, the local space consumed does not monotonically
increase anymore. 

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../paper"
%%% End:
