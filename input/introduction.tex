 
\section{Introduction}

\IEEEPARstart{C}{ausal} broadcast constitutes the core communication primitive
of many distributed systems~\cite{hadzilacos1994modular}. Applications such as
distributed social networks~\cite{borthakur2013petabyte}, distributed
collaborative software~\cite{heinrich2012exploiting,nedelec2016crate}, or
distributed data
stores~\cite{bailis2013bolton,bravo2017saturn,demers1987epidemic,lloyd2011cops,shapiro2011comprehensive}
use causal broadcast to ensure consistency criteria.  Causal broadcast ensures
reliable receipt of broadcast messages, and exactly-once delivery following
Lamport's happen before relationship~\cite{lamport1978time}. When Alice comments
on Bob's picture, nobody sees Alice's comment without Bob's picture, and nobody
sees multiple occurrences of Alice's comment or Bob's picture.
% If the sending of a message $m$ precedes the sending of a message $m'$ then all
% processes that deliver these two messages need to deliver $m$ before
% $m'$. Otherwise they deliver them in any order.

However, causal broadcast implementations do not scale in large and dynamic
systems comprising from hundreds to millions of processes joining, leaving, and
self-reconfiguring at any time. While ensuring causal delivery can be
lightweight~\cite{nedelec2018pcbroadcast}, forbidding multiple delivery is
overly space consuming. Every process delivers every message exactly-once by
either 
\begin{inparaenum}[(i)]
\item building a specific dissemination topology such as a tree or a ring that
  ensures single receipt hence single
  delivery~\cite{bravo2017saturn,raynal2013distributed}. However, such
  topologies are difficult to maintain in dynamic systems subject to
  failures~\cite{krasikova2016hashtable};
\item or they maintain a local structure to identify and discard additional
  copies of an original message at receipt. In asynchronous systems, these
  structures use logical clocks~\cite{malkhi2007concise,mukund2014optimized}.
  However, such structures suffer from linearly and monotonically increasing
  size with the number of processes that ever broadcast a message.
  Logical-clock-based approaches eventually become inefficient, for processes
  cannot safely reclaim entries without running an overcostly distributed
  garbage collecting protocol~\cite{abdullahi1998garbage}. State-of-the-art
  causal broadcast implementations do not scale in large and dynamic systems.
\end{inparaenum}

\begin{table*}
  \begin{center}
    \caption{\label{table:complexity} Complexity of broadcast algorithms at each
      process (detailed in Section~\ref{subsec:complexity}). $N$ the number of
      processes that ever broadcast a message. $P$ the number of processes in
      the system. $W$ the number of messages received but not delivered
      yet. $Q_i$ is the number of incoming links. $M$ is the number of
      messages already delivered that will be received again from at least one
      link in $Q_i$.}
  \input{input/tablecomplexity.tex}
  \end{center}
\end{table*}

In this paper, we introduce an implementation of causal broadcast that belongs
to approaches forbidding multiple delivery by using local structures. A process
belongs to an overlay network and has neighbors to communicate with via incoming
and outgoing links.  Broadcast efficiently disseminates messages by
gossiping~\cite{jelasity2007gossip}: the broadcaster sends the message by its
outgoing links; each process receiving such message forwards it by its own
outgoing links. Processes receive the message either directly or
transitively. Our contribution is threefold:
\begin{itemize}[leftmargin=*]
\item We define link memory as a mean for each process to forbid multiple
  delivery using a non-monotonic local structure. Link memory allows each
  process to identify neighbors that will send a broadcast message while
  removing obsolete control information about broadcast messages that will never
  be received again. Assuming causal order, we prove that each process can build
  such knowledge even in dynamic systems where processes join, leave, or
  self-reconfigure their neighborhood at any time.
\item Our implementation provides a novel tradeoff that actually depends on
  the system and its current usage. Quiescent systems enjoy no
  overhead. Table~\ref{table:complexity} shows that it keeps constant size
  overhead on messages while its space consumption is non-monotonic. It exploits
  causal order to purge all and only obsolete control information. This costs
  only a few lightweight control messages in dynamic systems.  Our implementation
  does not maintain any specific topology. However, the number of control
  messages depends on routing capabilities of the system and its dynamicity. For
  instance, gossip-based peer-sampling
  protocols~\cite{jelasity2007gossip,jelasity2009tman,nedelec2017adaptive} need
  only 8 control messages per added link. % during periodic shuffles.
\item Our experiments highlight the space consumed and the traffic generated by
  our protocol in dynamic systems with varying latency. The results confirm that
  the proposed approach scales with the system settings instead of past
  broadcast messages.
\end{itemize}
Instead of encoding the global state of the system in a vector of logical
clocks, our implementation only maintains its receipt state with its direct
neighborhood. Neighborhood being far smaller than the full system membership,
this scales in large and dynamic systems.  Our implementation offers an
advantageous tradeoff that makes causal broadcast a lightweight and efficient
middleware in large and dynamic systems. This tradeoff even makes a lightweight
and efficient implementation of reliable broadcast.

The rest of this paper is organized as follows. 
% Section~\ref{sec:motivations}
% shows the issue and motivates this work.
Section~\ref{sec:proposal} describes the model, highlights the issue, introduces
the principle solving the issue, provides an implementation solving the issue
along with its complexity analysis. Section~\ref{sec:experimentation} shows the
experiments. Section~\ref{sec:relatedwork} reviews related work. We conclude in
Section~\ref{sec:conclusion}.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../paper"
%%% End:
