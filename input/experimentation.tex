
\section{Experimentation}
\label{sec:experimentation}

\begin{figure*}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{./img/overhead.eps}
    \caption{\label{fig:overhead}Local space overhead over time consumed by
      \RPCBROADCAST to ensure causal order and forbid double delivery in dynamic
      systems with varying latency.}
  \end{center}
\end{figure*}



\RPCBROADCAST proposes an original tradeoff in terms of complexity. Among other,
it removes the last monotonic and linear upper bound that remained on space
complexity. In this section, we evaluate the impact of the actual system on the
space consumed and traffic generated by processes. The experiments run on the
\PEERSIM simulator~\cite{montresor2009peersim} that allows to build large and
dynamic systems. Our implementation is available on the Github platform at
\url{http://github.com/chat-wane/peersim-prcbroadcast}. 


% \begin{figure}
%   \begin{center}
%   \input{input/figtimelinebirpcbroadcast.tex}
%   \caption{\label{fig:bibroadcast}\RPCBROADCAST ensures the safety of bidirectional links
%     at marginal cost. Process~A maintains an additional buffer. Process~B transmits its
%     second buffer.}
%   \end{center}
% \end{figure}

\noindent \textbf{Objective:} To confirm that local space complexity
depends on in-views and message receipts.

\noindent \textbf{Description:} We measure the average size of buffers and
arrays of expected messages. This constitutes the average local space overhead
consumed by \RPCBROADCAST to detect and forbid multiple delivery in dynamic
systems.\\
Runs involve 3 overlay networks comprising 100, 1k, and 10k
processes. \SPRAY~\cite{nedelec2017adaptive} builds an highly dynamic overlay
networks that allows balancing the traffic generated by broadcasting among
processes. Each process maintains a neighborhood logarithmically scaling with
the number of processes in the system. Each process of the 100-processes system
has a neighborhood of $\approx 10$ neighbors. Each process of the 1k-processes
system has a neighborhood of $\approx 13.5$ neighbors. Each process of the
10k-processes system has a neighborhood of $\approx 15$ neighbors. Each process
dynamically re-configures their neighborhood every minutes by exchanging safe
links with a chosen neighbor: it gives half of its safe links to the chosen
neighbor; the latter gives half of its safe links to the former as well. Each
exchange leads to link memory initialization and safety checks of the new links,
and removal of given links.\\ Links are bidirectional, their safety must be
checked in both directions but the overhead remains minor. Links have
transmission delay, i.e., the time between the sending of a message and its
receipt is not null. The experiments start with $1ms$ delay. At $15min$ the
delay starts to increase. At ~$17min$ links reach $300ms$ delay. At $40min$
links reach $2.5s$ delay and it stops increasing.\\
From $2min$ to $50min$, every seconds, 10 processes chosen uniformly at random
among all processes broadcast a message.

\noindent \textbf{Results:} Figure~\ref{fig:overhead} shows the results of this
experiment. The x-axis denotes the time in minute. The top part of the figure
shows the local space overhead while the bottom part of the figure shows the
evolution of transmission delays.

\begin{figure*}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{./img/controlmessages.eps}
    \caption{\label{fig:controlmessages}Traffic overhead generated by
      \RPCBROADCAST.  Average number of control messages received -- including
      forwarding -- per process for each second.}
  \end{center}
\end{figure*}



\begin{itemize}
\item Figure~\ref{fig:overhead} confirms that the local space consumption
  depends on the in-view size. Systems with larger in-views consume more
  space. Each new delivered message adds control information on each link of the
  in-view (see Algorithm~\ref{algo:reliablebroadcast}).
\item Figure~\ref{fig:overhead} confirms that the local space consumption
  depends on network condition. The overhead increases as the latency
  increases. Latency increases the time between the first and the last receipt
  of each message. Processes store messages longer until they can be safely
  removed. 
\item Figure~\ref{fig:overhead} confirms that the local space consumption
  depends on broadcast messages. When processes stops broadcasting, the space
  consumed at each process drops to 0. Each process eventually receive each
  message and safely remove the corresponding entry.
\item Figure~\ref{fig:overhead} shows that at a rate of 10 broadcasts per second
  and when latency stays under a realistic bound ($300ms$), the overhead is
  lower than vector-based approaches. Whatever system conditions, it would
  require a vector of 100, 1k entries, 10k entries to forbid multiple delivery
  in the 100-processes system, 1k-processes system, 10k-processes system
  respectively. However, it is worth noting that the overhead of \RPCBROADCAST
  increases linearly with the number of messages currently transiting. 100
  broadcasts per second would multiply measurements made on \RPCBROADCAST by a
  factor of 10. In such case, the 100-entries vector would be better than
  \RPCBROADCAST even under $300ms$-latency. 
\end{itemize}

\noindent \RPCBROADCAST provides an original tradeoff where space complexity is
non-monotonic and actually depends on the system and its current usage; instead
of past usage using vectors of logical clocks. This result means that it
constitutes an advantageous tradeoff in
\begin{inparaenum}[(i)]
\item dynamic systems
\item comprising up to millions of processes
\item that could broadcast at any time.
\end{inparaenum} \\

\noindent \textbf{Objective:} To confirm that the generated traffic overhead
depends on the dynamicity of the system.

\noindent \textbf{Description:} We measure the average number of control
messages received by each process during a second. This includes the routing of
messages. The setup is identical to that of prior experiment.

\noindent \textbf{Results:} Figure~\ref{fig:controlmessages} shows the results of
this experiment. The top part of the figure depicts the traffic overhead
generated by \RPCBROADCAST while the bottom part of the figure depicts the
evolution of transmission delays.

\begin{itemize}
\item Figure~\ref{fig:controlmessages} shows that the number of control messages
  received by processes depends on the dynamicity of the system. The more
  dynamic the higher the traffic overhead. At the beginning of the experiment,
  processes join the system. Numerous links are established at once, hence the
  high number of control messages. Then processes shuffle their out-view during
  50 minutes. The number of links to add and remove is roughly constant over
  time, hence the stabilization in number of control messages. Finally,
  processes stop shuffling at $50min$. Processes do not receive additional
  control messages.
\item Figure~\ref{fig:controlmessages} confirms our traffic overhead complexity
  analysis. For instance, in the 10k-processes system, views comprises 15
  processes which belong half from the out-view and half from the in-view.  Each
  process shuffles every minutes. Each shuffle adds and removes 7.5 links (twice
  half of the out-view size). Since the peer-sampling protocol establishes links
  using neighbor-to-neighbors interactions, it allows a form of routing where
  only 8 control messages are required to initialize a new link.
  $|exchanged\_links|*|control\_messages|/60 \approx 7.5*8/60 \approx 1$ control
  message per second.
\item Figure~\ref{fig:controlmessages} shows that latency smooth and decreases
  the number of control messages. The peer-sampling protocol only shuffles links
  already safe and the memory of which is initialized. Since increasing latency
  increases the initialization time of links, processes exchange less links at
  each shuffle. The generated traffic decreases accordingly. Latency also
  spreads control messages over time, hence the smoothing in measurements.
\end{itemize}

% Overall, this section empirically confirms that the space complexity of
% \RPCBROADCAST is non-monotonic and depends on the system and its
% usage. Section~\ref{subsec:complexity} states that the space complexity is
% $O(Q_i.M)$ where $Q_i$ is the size of the in-view built by peer-sampling
% protocols, and $M$ is the number of messages that have been delivered but that
% will be received again. This experiment confirms that the space consumed by each
% process increases and decreases over time. In dynamic settings, this comes at
% the cost of control messages to initialize memory link.

\noindent Assuming peer-sampling protocols that enable a form of routing,
\RPCBROADCAST forbids multiple delivery at the cost of few lightweight control
messages in dynamic systems. In this experiment, the underlying peer-sampling
protocol builds random graph topologies that have numerous desirable properties
such as crash resilience, or load balancing~\cite{jelasity2007gossip}. This is
ideal for systems where numerous processes join and leave continuously. However,
it is worth noting that it remains in the hands of developers to choose the best
peer-sampling protocol for their system (e.g. minimizing
latency~\cite{dabek2004vivaldi}).


Overall, this section showed that \RPCBROADCAST proposes an original tradeoff
that actually depends on the system (its dynamicity, its latency, its topology)
and current usage (broadcasts per second).  \RPCBROADCAST achieves to forbid
multiple delivery and to safely remove obsolete control information about
broadcast messages. The next section reviews state-of-the-art approaches
designed to forbid multiple delivery.




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../paper"
%%% End:
